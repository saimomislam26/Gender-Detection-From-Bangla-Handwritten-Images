{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "160204011_problem#2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaAG8Lu0lUOO"
      },
      "source": [
        "## **Ekush Bengali Handwritten Digits**\n",
        "\n",
        "Handwritten Bangla digit recognition is one of the most challenging computer vision problems due to its diverse shapes and writing style including the label male or female.using neural network we have to find that whether the bangla digit is of male or is of a female.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1XtftbqV48mEFN6646XoKPN-mcWVc_JDz\" width=\"400\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fKb5x6UbBVY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYdi4OKxhDoB",
        "outputId": "9d3355d2-b737-4d7a-a9b3-2243f416ccac"
      },
      "source": [
        "from google.colab import drive #mounted all file and folder from google drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6EsGZTghUo6"
      },
      "source": [
        "Root ='/content/gdrive/My Drive/Softcom/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G7sGNGXlg26"
      },
      "source": [
        "**function for read the csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75q9YEJ2cKiQ"
      },
      "source": [
        "def showRawTrainingSamples(csv_filename):\n",
        "  if csv_filename == 'maleDigits.csv':\n",
        "    df1 = pd.read_csv(Root+csv_filename)\n",
        "    print(csv_filename)\n",
        "    print(df1.columns)\n",
        "    return df1\n",
        "  else:\n",
        "    df2 = pd.read_csv(Root+csv_filename)\n",
        "    print(csv_filename)\n",
        "    print(df2.columns) \n",
        "    \n",
        "  return df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA28URWBh7SN"
      },
      "source": [
        "male_csv = showRawTrainingSamples('maleDigits.csv')\n",
        "female_csv = showRawTrainingSamples('femaleDigits.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw2JP_uOlnsY"
      },
      "source": [
        "**giving the level and merging the csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UJP4GmcjLN"
      },
      "source": [
        "male_csv['label'] = 1\n",
        "female_csv['label'] = 0\n",
        "female_label=male_csv['label']\n",
        "male_label=female_csv['label']\n",
        "df_names=[female_label,male_label]\n",
        "all_labels=pd.concat(df_names,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhY3rhfAdBgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3d4405-ac95-404b-eaf7-89d5aa6f6540"
      },
      "source": [
        "df_name=[male_csv,female_csv]\n",
        "total_image=pd.concat(df_name,ignore_index=True)\n",
        "total_image=total_image.drop(labels='label',axis=1)\n",
        "print(total_image.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30830, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ_zHMuHeqM1"
      },
      "source": [
        "total_image=total_image/255.0\n",
        "total_image=total_image.values.reshape(-1,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Lh7tKoltpH"
      },
      "source": [
        "**showing image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "jeSRUqB6fBt9",
        "outputId": "ff4b32f5-f761-40de-c575-f8019caa9ad6"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "\n",
        "show_img = total_image[0].reshape(28, 28)\n",
        "plt.imshow(show_img, cmap='gray')\n",
        "print(all_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R02ThzBvfFGd"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(total_image, all_labels, test_size=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biE0zczh2bwm"
      },
      "source": [
        "train_dataset = list(zip(X_train,y_train))\n",
        "test_dataset = list(zip(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0lrHsWl5RF"
      },
      "source": [
        "### Hyperparameter initialazation\n",
        "Hyper parameter is predefined which is not changed during run time.But it is an important factor. we must be careful about choosing the hyperparameter value.\n",
        "Here batch size,number of iteration,input dimension,output dimension,learning rate are Hyperparameters.Even choosing the optimizer is also a hyperparameter.we chose loss Entropy instead of MSE here for better output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxzZk12em14i"
      },
      "source": [
        "- **totaldata:** 30830\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3,000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{30830}{100} = 9$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Lgkk_om_RI"
      },
      "source": [
        "### Construct loss and optimizer (select from PyTorch API)\n",
        "\n",
        "Unlike linear regression, we do not use MSE here, we need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss() ` \n",
        "\n",
        "It does 2 things at the same time.\n",
        "\n",
        "1. Computes softmax **([Logistic or Sigmoid]/softmax function)**\n",
        "2. Computes Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1dfEDVQ23LI",
        "outputId": "64bf5ebc-fcb7-4735-a358-da844701df35"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "#num_hidden = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = 128)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float())\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images.float())\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.7573212385177612. Accuracy: 51.3136555303276\n",
            "Iteration: 1000. Loss: 0.6899835467338562. Accuracy: 48.6863444696724\n",
            "Iteration: 1500. Loss: 0.698710560798645. Accuracy: 48.6863444696724\n",
            "Iteration: 2000. Loss: 0.6932975053787231. Accuracy: 51.3136555303276\n",
            "Iteration: 2500. Loss: 0.6991208791732788. Accuracy: 51.3136555303276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ysQn5LnMN0"
      },
      "source": [
        "**Setting2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtbPfLvBnko8"
      },
      "source": [
        "- **totaldata:** 30830\n",
        "- **minibatch:** 200\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3,000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{30830}{200} = 19$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzAKxI010yMY",
        "outputId": "dde9f987-e7fd-4d3e-fec3-a58971839d18"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 200\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = 128)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float())\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images.float())\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6792611479759216. Accuracy: 62.082387285111906\n",
            "Iteration: 1000. Loss: 0.6312875151634216. Accuracy: 63.282517028867986\n",
            "Iteration: 1500. Loss: 0.5901315808296204. Accuracy: 62.40674667531625\n",
            "Iteration: 2000. Loss: 0.5244690179824829. Accuracy: 60.07135906584496\n",
            "Iteration: 2500. Loss: 0.41976356506347656. Accuracy: 59.098280895231916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYCvxTRXn_xP"
      },
      "source": [
        "**Setting 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLVPY8I7nvrj"
      },
      "source": [
        "- **totaldata:** 30830\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3,000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{30830}{100} = 9$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8M0sL41OJC",
        "outputId": "bd57e5f6-93de-4cbb-a159-328c71b8a066"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "\n",
        "        ### 5th hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = 128)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float())\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images.float())\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.703399658203125. Accuracy: 51.3136555303276\n",
            "Iteration: 1000. Loss: 0.6970860958099365. Accuracy: 48.6863444696724\n",
            "Iteration: 1500. Loss: 0.6905996799468994. Accuracy: 48.6863444696724\n",
            "Iteration: 2000. Loss: 0.6950985789299011. Accuracy: 51.3136555303276\n",
            "Iteration: 2500. Loss: 0.6940391063690186. Accuracy: 48.6863444696724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBv7A8ghoTQ3"
      },
      "source": [
        "**Setting 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syIqqfuEpU2s"
      },
      "source": [
        "- **totaldata:** 30830\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 6,000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{30830}{100} = 19$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WAsc2FJ3ACX",
        "outputId": "7c2c420b-d486-4464-9467-cd347250d2cb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 6000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 6th hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 6th hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "\n",
        "        ### 5th hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "\n",
        "        ### 6th hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 6th hidden layer\n",
        "        out = self.relu_6(out)\n",
        "\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = 128)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float())\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images.float())\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6763009428977966. Accuracy: 59.26046059033409\n",
            "Iteration: 1000. Loss: 0.7243668437004089. Accuracy: 62.179695102173206\n",
            "Iteration: 1500. Loss: 0.6588746905326843. Accuracy: 62.504054492377556\n",
            "Iteration: 2000. Loss: 0.5687410831451416. Accuracy: 62.27700291923451\n",
            "Iteration: 2500. Loss: 0.5808610916137695. Accuracy: 62.69867012650016\n",
            "Iteration: 3000. Loss: 0.5541413426399231. Accuracy: 62.66623418747973\n",
            "Iteration: 3500. Loss: 0.49485939741134644. Accuracy: 62.147259163152775\n",
            "Iteration: 4000. Loss: 0.5221499800682068. Accuracy: 62.63379824845929\n",
            "Iteration: 4500. Loss: 0.5865119695663452. Accuracy: 61.59584819980538\n",
            "Iteration: 5000. Loss: 0.5273468494415283. Accuracy: 62.47161855335712\n",
            "Iteration: 5500. Loss: 0.4258202612400055. Accuracy: 60.91469348037626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5M3TnnBp0bY"
      },
      "source": [
        "**Setting 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLANiZTrpyNG"
      },
      "source": [
        "- **totaldata:** 30830\n",
        "- **minibatch:** 50\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3,000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{30830}{100} = 4$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f7YhTtVpi1P",
        "outputId": "b6079406-58fd-436e-9803-daec11755ea6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 6000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        ### 6th hidden layer: 100 --> 100\n",
        "        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 6th hidden layer\n",
        "        self.relu_6 = nn.ReLU()\n",
        "\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 4th hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "\n",
        "        ### 5th hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 5th hidden layer\n",
        "        out = self.relu_5(out)\n",
        "\n",
        "\n",
        "        ### 6th hidden layer\n",
        "        out  = self.linear_6(out)\n",
        "        ### Non-linearity in 6th hidden layer\n",
        "        out = self.relu_6(out)\n",
        "\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = 128)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float())\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images.float())\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.7081325054168701. Accuracy: 61.30392474862147\n",
            "Iteration: 1000. Loss: 0.7107574939727783. Accuracy: 60.55789815115148\n",
            "Iteration: 1500. Loss: 0.6062929034233093. Accuracy: 62.04995134609147\n",
            "Iteration: 2000. Loss: 0.6201570630073547. Accuracy: 62.76354200454103\n",
            "Iteration: 2500. Loss: 0.5034205913543701. Accuracy: 62.63379824845929\n",
            "Iteration: 3000. Loss: 0.5552213788032532. Accuracy: 61.466104443723644\n",
            "Iteration: 3500. Loss: 0.5344298481941223. Accuracy: 61.07687317547843\n",
            "Iteration: 4000. Loss: 0.32604697346687317. Accuracy: 61.53097632176451\n",
            "Iteration: 4500. Loss: 0.3501764237880707. Accuracy: 60.49302627311061\n",
            "Iteration: 5000. Loss: 0.4791857898235321. Accuracy: 60.72007784625365\n",
            "Iteration: 5500. Loss: 0.371969074010849. Accuracy: 61.66072007784626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZKk8EqokiR8"
      },
      "source": [
        "###Result\n",
        "\n",
        "|Setting| Batch Size       | No. of iteration     | Learning Rate     |\n",
        "| :------------- | :----------: | -----------: |:------------- |\n",
        "|           |\n",
        "|1|  100 | 3000   | .001    |\n",
        "|2| 200   | 3000 | .001|\n",
        "|3| 100|3000 |.1 |\n",
        "|4| 100|6000 |.001 |\n",
        "|5|50|3000|.001||\n",
        "\n",
        "in setting 1 output was approximately 51% but remaining the same value for all hypermeter except batch size.batch size was increased then the accuracy was almost same.approximately 62-63%% then i tried different hypermeter,for every changes accuracy was almost same.but when i toke batch size 200 instead of 100 for second iteration accuracy was increased a bit ."
      ]
    }
  ]
}